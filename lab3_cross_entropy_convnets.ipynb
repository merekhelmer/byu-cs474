{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RySgAEoGve-m"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/porterjenkins/byu-cs474/blob/master/lab3_cross_entropy_convnets.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Deep Learning Part 3: Datasets, Data Loading, Cross Entropy, and Convolutional Networks\n",
        "\n",
        "## Grading Standards:\n",
        "*  10%: Dataset/MNIST section\n",
        "*  10%: Correct implementation and use of cross entropy loss\n",
        "*  20%: Correct training/validation functions\n",
        "*  40%: Successful training and validation with MLP and convolution networks\n",
        "*  12%: Convolutional layer quiz\n",
        "*   8%: Comparison between MLP and convolution networks\n",
        "___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wQOefmcZVgTl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUHlLRJ0Q9Fz"
      },
      "source": [
        "Set your global variable `device`, using the `torch.device()` function. In order to use cuda remember to request a GPU from Runtime > Change Runtime.\n",
        "\n",
        "***Important Note**: If you spend too much time or memory on the GPU in Google Colab then you will be timed out. This may not be a big deal with this lab, but it can become a big deal in later labs. It is recommended to set your `device` and Runtime to CPU first and once everything in the lab is working properly to set it to the GPU.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kFAeNl0mQ9F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572d355d-c865-4353-8973-91d3a8fef5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8plxzBbQ9F1"
      },
      "source": [
        "---\n",
        "\n",
        "# Datasets and Data Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6A1ZwnczQ9F1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP7rpMjiQ9F2"
      },
      "source": [
        "We are going to make a PyTorch `Dataset`.\n",
        "There are three parts to creating a `Dataset`:\n",
        "1. `__init__()`: This is where you get all relevant data for your dataset.\n",
        "2. `__len__()`: You return how large your dataset is.\n",
        "3. `__getitem__()`: You return an item from your dataset given an index.\n",
        "\n",
        "Implement the TODOs below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x3SGWrkbQ9F3"
      },
      "outputs": [],
      "source": [
        "class SineDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # TODO: The code is same from lab 2, so you can uncomment the code below.\n",
        "        self.x = torch.rand((100,1))*8 - 4\n",
        "        self.y = torch.sin(self.x) + torch.randn_like(self.x)*.1 # the second part of the sum adds noise to the function\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # TODO: i will be an index so return x_i and y_i\n",
        "        return self.x[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtRsLxWQ9F3"
      },
      "source": [
        "We will now create a `SineDataset` and print out the length of dataset, i.e. `len(dataset)`, and the item in your dataset at index 0, i.e. `dataset[0]`.\n",
        "\n",
        "*Note: `__len__()` and `__getitem__()` are private methods and should not be called directly.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M1HD_XafQ9F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7344af-7689-42e4-855c-5659d706d5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "(tensor([-0.4557]), tensor([-0.5204]))\n"
          ]
        }
      ],
      "source": [
        "dataset = SineDataset()\n",
        "print(len(dataset))\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ow7q2UEQ9F4"
      },
      "source": [
        "A `DataLoader` uses the `__len__` and `__getitem__` of a `Dataset` to sample indices in `[0, ..., len(dataset)-1]` and collect a batch of items from the `Dataset`.\n",
        "The `DataLoader` will then try to convert the sampled entries into tensors (if they are not already) and concatenate them together.\n",
        "Create a `DataLoader` object below; pass in your dataset and `batch_size=32` as arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8G9BkZUgQ9F4"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHE3O4wwQ9F5"
      },
      "source": [
        "Iterate through your dataloader with a for loop. Because `SineDataset.__getitem__()` returns two items, the for loop will return a tuple.\n",
        "Either unpack the entries in your tuple in the for loop:\n",
        "```python\n",
        "for x, y in dataloader\n",
        "```\n",
        "or after the loop:\n",
        "```python\n",
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "```\n",
        "\n",
        "Print out the shapes of `x` and `y` for each batch in the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zxJ-K52QQ9F5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de60d3ed-6165-4463-c98e-eadeeb4b365b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([32, 1]) torch.Size([32, 1])\n",
            "torch.Size([4, 1]) torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "for batch in dataloader:\n",
        "    x, y = batch\n",
        "    print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9XGi82cQ9F5"
      },
      "source": [
        "You will notice that the shapes are `(B, Z_in)`, where `B` is batch size and `Z_in` is our input feature size, which is exactly what we want.\n",
        "Also note that the last batch has a batch size of 4; this is because the `DataLoader` samples **without replacement** and these are the last items in our `Dataset` that have not been sampled.\n",
        "\n",
        "Let's now create our real dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORW8H-QwQ9F6"
      },
      "source": [
        "---\n",
        "\n",
        "# MNIST\n",
        "\n",
        "We are now going to look at the MNIST dataset, which is a dataset of handwritten numbers.\n",
        "Our objective will be to create a neural network that can predict the number given the image.\n",
        "\n",
        "First import `torchvision` below so we can retrieve the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ybgv1p4PQ9F6"
      },
      "outputs": [],
      "source": [
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QroZnsvvQ9F7"
      },
      "source": [
        "You can use `torchvision.datasets.MNIST()` to download the MNIST dataset (which inherits the `Dataset` class).\n",
        "For arguments, specify `root=\"/tmp/\"` to denote the location, `train=True` or `train=False` to get the training or test dataset, `download=True` to specify you want to download the dataset, and `transform=torchvision.transforms.ToTensor()` to convert the MNIST images from PIL images to PyTorch tensors.\n",
        "Create both a `train_dataset` and `val_dataset`.\n",
        "\n",
        "*Note: It is good practice to use a train, val, and test dataset, especially in the real world, but in this class we will mainly focus on train and val datasets to simplify things.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BeJcijXkQ9F7"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root=\"/tmp/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
        "val_dataset = torchvision.datasets.MNIST(root=\"/tmp/\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujysG9TCQ9F7"
      },
      "source": [
        "Print out the lengths of `train_dataset` and `val_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "61Px30_LQ9F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c142994e-bd4d-46f0-99fc-d5dade637802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fViNzhuQ9F8"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Grab element 0 from the `train_dataset`. As a heads up, like our `SineDataset`, `MNIST` returns an image `x` and a class/target `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3QUod3d5Q9F8"
      },
      "outputs": [],
      "source": [
        "x, y = train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEJ45xaVQ9F8"
      },
      "source": [
        "Use the `type()` function to see what type of object `x` and `y` are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BH7jijNxQ9F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a261e41-83bb-46c4-d6a9-aeab22ee0c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "print(type(x), type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8xFEKjZQ9F9"
      },
      "source": [
        "Since x is a tensor, print out its `.dtype`, `.shape`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nspQc1uSQ9F9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32f9227-8607-4b53-d355-473feec4cf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32 torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(x.dtype, x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-K1HjMQ9F9"
      },
      "source": [
        "The shape of a tensor image is `(C, H, W)`, where `C` is channels, `H` is height, and `W` is width.\n",
        "In our case `x` has 1 channel and it is a 28x28 image.\n",
        "Because there is 1 channel, it is likely that the image is grayscale (which it is).\n",
        "\n",
        "Now visualize the image and display its class.\n",
        "Use the `plt.imshow()` function to visualize `x`; add the argument `cmap=\"gray\"` to denote the image is grayscale.\n",
        "Use the `plt.title()` function to set the title of the `plt` image to the class `y`.\n",
        "\n",
        "*Note: `plt` expects grayscale images to only have to dimension, HxW, so `.squeeze()` the 0th dimension*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Xq6qHfNZQ9F9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "a461311b-a851-4ad3-9354-e500c7c9a782"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(x.squeeze(), cmap=\"gray\")\n",
        "plt.title(y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ7Qc7N5Q9F9"
      },
      "source": [
        "Print out the min and max values of `x` using `torch.min()` and `torch.max()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0fU7fzKGQ9F-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44a331d-7d15-402a-894e-bd9cb1186e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.) tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "print(torch.min(x), torch.max(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLK7CH_YQ9F-"
      },
      "source": [
        "Our tensors are normalized between 0 and 1, which is good so we don't have to do any normalization.\n",
        "Now that we have a better understanding of our image data, let's examine the classes in the dataset.\n",
        "\n",
        "Create a `get_dataset_classes()` function which takes a dataset as input and count how many times each class appears.\n",
        "Return a dictionary where the keys are the classes and the values represent the number of times each class appears in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RLGxbOj3Q9F-"
      },
      "outputs": [],
      "source": [
        "def get_dataset_classes(dataset):\n",
        "    classes = {}\n",
        "    for _, y in dataset:\n",
        "        if y in classes:\n",
        "            classes[y] += 1\n",
        "        else:\n",
        "            classes[y] = 1\n",
        "    return classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNlLGb2pQ9F-"
      },
      "source": [
        "Execute the code below to visualize the dataset classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3ZoP7o0JQ9F_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "63cff02f-26ea-43d0-af82-277a480d2ea2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_classes = get_dataset_classes(train_dataset)\n",
        "val_classes = get_dataset_classes(val_dataset)\n",
        "\n",
        "def plot_classes(ax, classes, title):\n",
        "    ax.bar(classes.keys(), classes.values())\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(list(range(10)))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "plot_classes(axes[0], train_classes, \"Train Dataset Classes\")\n",
        "plot_classes(axes[1], val_classes, \"Val Dataset Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfg-UTydQ9F_"
      },
      "source": [
        "There are 10 classes, which makes sense because there are 10 digits, 0-9.\n",
        "Furthermore, the classes are pretty evenly spread on both datasets, which means our baseline accuracy is roughly 10%.\n",
        "\n",
        "Now create two `DataLoader` objects called, `train_loader` and `val_loader`.\n",
        "The `train_loader` should have your `train_dataset` and the `val_loader` should have your `val_dataset`.\n",
        "Set the `batch_size` of both dataloaders equal to 32 and set `shuffle=True` for the `train_loader` so that the dataset is shuffled every time.\n",
        "To improve the speed at which you `DataLoader`s can load the data, set `num_workers=4` (for multiprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mLp4yWwXQ9F_"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEGFonGnQ9GH"
      },
      "source": [
        "Now use `x, y = next(iter(train_loader))` to get a single batch of data from the `train_loader` and print out the shapes and dtypes of `x` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b_XsyFmUQ9GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6a1f85-8fb5-4905-c297-3a668483bf7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28]) torch.float32 torch.Size([32]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "x, y = next(iter(train_loader))\n",
        "print(x.shape, x.dtype, y.shape, y.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpSxXIkQQ9GI"
      },
      "source": [
        "The shape of `x` is `(B, C, H, W)`, where `B` is the batch size. **Always remember, in PyTorch, your data should have a batch dimension.**\n",
        "The shape of `y` is `(B,)` and it is a tensor of type `long`, which is what we want because we are doing classification, which means we want our target to be a class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY4owfQwm-Ni"
      },
      "source": [
        "___\n",
        "\n",
        "### MLP Network\n",
        "\n",
        "We are now going to make our network.\n",
        "Because we are doing image classification, the input to our network is a batch of images, `shape=(B, C, H, W)`, and the output of our network is a batch of probabilities, `shape=(B, K)`, where `K` represents the number classes in our dataset.\n",
        "In our case `K=10`.\n",
        "\n",
        "We will first try to solve this problem using a fully connected deep network (like your DeepNet from lab 2), sometimes called a Multi-Layer Perceptron (MLP).  \n",
        "Implement an `MLP` below (don't forget to use `nn.Sequential`, `nn.Linear`, `nn.ReLU`).\n",
        "Because `nn.Linear` expects tensors of shape `(B, Z)`, where `Z` is the input feature size, we need to flatten our images. Use the `.view()` function to reshape `x.shape=(B, C, H, W)` into `x.shape=(B, Z)`, where `Z=C*H*W`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ljx0drnxQ9GI"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, hidden_size):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJIluYrhQ9GI"
      },
      "source": [
        "Because we are doing classification, we need to use a different loss function that MSE; cross entropy loss is a good choice and is a common loss function for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trSdeZy2Q9GJ"
      },
      "source": [
        "---\n",
        "\n",
        "# Softmax and CrossEntropy\n",
        "\n",
        "The cross entropy (CE) function is $CE(p, q) = - \\sum p(x) \\log q(x)$, where $p$ and $q$ are probability functions ($p$ is the target probabilities and $q$ is the predicted probabilities) and in our case $x$ represents a class.\n",
        "\n",
        "$p$ represents the target distribution, the true class distribution, which means it is a one-hot vector $p_c$, where $c$ represents the index of the class:\n",
        "$$p_i = \\begin{cases}\n",
        "1, & \\textrm{if } i = c \\\\\n",
        "0, & \\textrm{if } i \\not = c \\\\\n",
        "\\end{cases}$$\n",
        "\n",
        "Then $CE(p, q) = - \\sum_i p_i \\log q_i$ will become $CE(p, q) = - p_c \\log q_c$, because $p_{i \\not = c} = 0$, which is further reduced to $CE(p, q) = - \\log q_c$, since $p_c = 1$. In other words, cross entropy loss for classification is the negative log of the predicted probability of the correct class.\n",
        "\n",
        "Therefore, $p$ is never passed into `F.cross_entropy()`, instead you pass in $q$ you predicted distribution and $c$ the index of the correct class.\n",
        "\n",
        "Implement `prenormalized_cross_entropy_loss` below. $q$ is assumed to be a normalized probability distribution.\n",
        "\n",
        "*Note 1: Do **not** use a for loop. You can index into a tensor with array slicing (hint: You will need to use `torch.arange()` for the 0th dimension of `q`)*\n",
        "\n",
        "*Note 2: Compute the mean cross entropy of the batch not the sum*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xuhEjtx7Q9GJ"
      },
      "outputs": [],
      "source": [
        "def prenormalized_cross_entropy_loss(q, c):\n",
        "    return -torch.mean(torch.log(q[torch.arange(len(q)), c]))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqzoiQDgQ9GK"
      },
      "source": [
        "Validate your function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5l96Vh8HQ9GK"
      },
      "outputs": [],
      "source": [
        "def test_prenormalized_cross_entropy_loss():\n",
        "    q = torch.tensor([[.1, .5, .4],\n",
        "                      [.2, .2, .6],\n",
        "                      [.3, .3, .3]])\n",
        "    c = torch.tensor([2, 0, 1])\n",
        "    assert torch.allclose(prenormalized_cross_entropy_loss(q, c), torch.tensor(1.2432), atol=1e-4)\n",
        "\n",
        "test_prenormalized_cross_entropy_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4pxw0AhQ9GL"
      },
      "source": [
        "This assumed that `q` was a probability distribution, but usually neural networks output logits $l$, which are unnormalized probabilities.\n",
        "One way we could normalize our logits $l$ into probabilities $q$ is to divide $l$ by the sum of logits $q_i = \\frac{l_i}{\\sum_j l_j}$, but that doesn't work if $l_i$ is negative.\n",
        "The softmax, which exponentiates the $logit$, $q_i = \\frac{e^{l_i}}{\\sum_j e^{l_j}}$, before dividing by the sum of exponentiated logits removes the issues of negativity (there are other good reasons for using softmax, such as numerical stability).\n",
        "\n",
        "However, applying `q = softmax(l)` to `prenormalized_cross_entropy_loss(q, c)` can still be numerically unstable.\n",
        "Luckily, we can simplify our function:\n",
        "$$\\begin{align}\n",
        "CE(l, c) &= - \\log \\frac{e^{l_c}}{\\sum_j e^{l_j}} \\\\\n",
        "&= - (\\log e^{l_c} - \\log \\sum_j e^{l_j}) \\\\\n",
        "&= - (l_c - \\log \\sum_j e^{l_j}) \\\\\n",
        "&= - l_c + \\log \\sum_j e^{l_j}\n",
        "\\end{align}$$\n",
        "\n",
        "While you could implement $\\log \\sum_j e^{l_j}$, you should use `torch.logsumexp()` which will exponentiate, sum, and then log your logits, but in a more numerically stable way.\n",
        "Implement `cross_entropy_loss()` below.\n",
        "You can validate it works by comparing it with the output of `F.cross_entropy()`.\n",
        "\n",
        "*Note: Do **not** use a for loop*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eqBTmzaWQ9GL"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(l, c):\n",
        "    batch_size = l.size(0)\n",
        "    correct_logits = l[torch.arange(batch_size), c]\n",
        "    log_sum_exp = torch.logsumexp(l, dim=1)\n",
        "\n",
        "    loss = -correct_logits + log_sum_exp\n",
        "    return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBEOn1gKQ9GL"
      },
      "source": [
        "It is hard to tell how well a model is performing just from its cross entropy loss, so create a `get_accuracy()` function to measure accuracy.\n",
        "`get_accuracy()` takes in a `y_hat` and `y`, where `y_hat` contains the predicted logits (unnormalized probabilities) for some images `x` and `y` are the labels.\n",
        "You can get the predicted label from `y_hat`, by using the `torch.argmax()` function.\n",
        "\n",
        "*Note: Do **not** use a for loop.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "aDoJ9UewQ9GL"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(y_hat, y):\n",
        "    predicted = torch.argmax(y_hat)\n",
        "    return torch.mean((predicted == y).float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gliu8YgjQ9GL"
      },
      "source": [
        "## Validation and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdjBaAfIve-4"
      },
      "source": [
        "To see how well training is going, implement a `validation()` function to compute the average loss and accuracy over all instances in the `val_loader`.\n",
        "This function will look very similar to a basic training loop, but without any optimization, e.g. no `loss.backward()` or `optimizer.step()`.\n",
        "To speed up the process use `torch.no_grad()` to keep PyTorch from building the computation graph.\n",
        "You can use `torch.no_grad()` either as a decorator:\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def fn():\n",
        "    ...\n",
        "```\n",
        "or as a context manager:\n",
        "```python\n",
        "def fn():\n",
        "    with torch.no_grad():\n",
        "        ...\n",
        "```\n",
        "\n",
        "*Remember you can use `.item()` on a tensor with one element to convert it into a float/int.*\n",
        "\n",
        "*Note: Use your*`cross_entropy` *function. Do **not** use* `F.cross_entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "eVpKK9_bQ9GM"
      },
      "outputs": [],
      "source": [
        "# TODO: Return the network's average loss and accuracy on the val loader\n",
        "def validation(net, val_loader):\n",
        "    net.eval()\n",
        "    run_loss = 0.0\n",
        "    run_accuracy = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_hat = net(x)\n",
        "            loss = cross_entropy_loss(y_hat, y)\n",
        "            run_loss += loss.item()\n",
        "\n",
        "            #accuracy per batch\n",
        "            _, preds = torch.max(y_hat, 1)\n",
        "            batch_accuracy = torch.mean((preds == y).float())\n",
        "            run_accuracy += batch_accuracy.item()\n",
        "\n",
        "            total_batches += 1\n",
        "\n",
        "    avg_loss = run_loss / total_batches\n",
        "    avg_acc = run_accuracy / total_batches\n",
        "\n",
        "    return avg_loss, avg_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RHZYEKNQ9GM"
      },
      "source": [
        "Now implement the `train()` function.\n",
        "This will look similar to the `train()` you implemented in lab 2, but now you will also store training accuracies, and at every `log_val_interval` you will call `validation` and store the validation loss and accuracy.\n",
        "\n",
        "*Note: Use your*`cross_entropy` *function. Do **not** use* `F.cross_entropy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7PQFG1zAQ9GM"
      },
      "outputs": [],
      "source": [
        "def train(net, train_loader, val_loader, optimizer, n_optimization_steps, log_val_interval):\n",
        "    # TODO: Implement training loop and return the training and validation losses and accuracies.\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for i in range(n_optimization_steps):\n",
        "        net.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_hat = net(x)\n",
        "            loss = cross_entropy_loss(y_hat, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            _, preds = torch.max(y_hat, 1)\n",
        "            batch_accuracy = torch.mean((preds == y).float())\n",
        "            train_accuracies.append(batch_accuracy.item())\n",
        "\n",
        "            if i % log_val_interval == 0:\n",
        "                val_loss, val_acc = validation(net, val_loader)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accuracies.append(val_acc)\n",
        "\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpKn33eFQ9GM"
      },
      "source": [
        "Now train an `MLP` on `MNIST`. To speed up training use the `torch.optim.Adam` optimizer instead of `torch.optim.SGD`.\n",
        "\n",
        "*Note: Don't forget to put your network on `device`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PaunNy19Q9GN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "collapsed": true,
        "outputId": "f8008f7a-c9fe-49cc-bfff-b910856bdf78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _releaseLock at 0x786f78839620>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
            "    def _releaseLock():\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3360090313.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# TODO: Train your MLP for 2000 steps and set log_val_interval=50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3106985341.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, val_loader, optimizer, n_optimization_steps, log_val_interval)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_val_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-874595723.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(net, val_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TODO: Set torch seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# TODO: Initialize your MLP, called net\n",
        "net = MLP(in_features=784, out_features=10, hidden_size=128).to(device)\n",
        "\n",
        "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=.001)\n",
        "\n",
        "\n",
        "# TODO: Train your MLP for 2000 steps and set log_val_interval=50\n",
        "train_losses, train_accuracies, val_losses, val_accuracies = train(net, train_loader, val_loader, optimizer, 2000, 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxlBHEY5Q9GN"
      },
      "source": [
        "Now plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot.\n",
        "Properly set your x- and y-axis labels and create a legend to make your plot legible.  \n",
        "\n",
        "*Note: that you can specify the x-values for each point by calling `plt.plot(x, y)` instead of `plt.plot(y)`. Since you store validation every 50 steps, you'll need to use `torch.arange` to get the proper x-values to align the validation results to the training results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "janqsjw6Q9GN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "outputId": "44327086-5cf5-41fd-a024-8fcb524389d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_losses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4018059616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S6LIPLPGfmq0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ronkEckHiDaU"
      },
      "source": [
        "___\n",
        "\n",
        "# Convolution Networks\n",
        "\n",
        "Now we will create a convolution network.\n",
        "When we were dealing with an `MLP` we used fully-connected `nn.Linear` layers.\n",
        "In a convolution network we use `nn.Conv2d` layers.\n",
        "`nn.Linear` maps tensors of shape `(B, F_in) -> (B, F_out)`.\n",
        "`nn.Conv2d` maps tensors of shape `(B, C_in, H_in, W_in) -> (B, C_out, H_out, W_out)`, where `C_in` represents our input channels and `C_out` represents our output channels.\n",
        "You decide what `C_out` should be when you initialize.\n",
        "The mapping that\n",
        "\n",
        "If we had a batch of `img` tensors with shape `(B, 3, 8, 8)` and we wanted it to become `(B, 6, 4, 4)` we could create a convolution layer:\n",
        "```python\n",
        "conv_layer = nn.Conv2d(in_channels=3,  # Our 'C_in' which is 3\n",
        "                       out_channels=6, # Our desired 'C_out'\n",
        "                       kernel_size=2,  # One way to make an 8x8 image become a 2x2 image is to have the kernel be 2x2,\n",
        "                       padding=0,      #    with zero padding,\n",
        "                       stride=2,       #    and a stride of 2.\n",
        "                      )\n",
        "```\n",
        "Validate this is true below by creating `conv_prac()` function, which creates a random tensor with shape `(B, 3, 8, 8)`, passes it through `conv_layer`, and prints out the resulting shape.\n",
        "\n",
        "*Note: `kernel_size`, `padding`, and `stride` can all be tuples in case you want different (height, width) parameters, e.g. `kernel=(kernel_height, kernel_width)`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0WFcZkkWQ9GN"
      },
      "outputs": [],
      "source": [
        "def conv_prac():\n",
        "    img = torch.rand((1, 3, 8, 8))\n",
        "    conv_layer = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=2, padding=0, stride=2)\n",
        "\n",
        "conv_prac()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4433c1d6"
      },
      "source": [
        "\n",
        "* * *\n",
        "\n",
        "### Quiz\n",
        "Test your knowledge of how convolution layers affect the shape of outputs by answering the following quiz questions.\n",
        "\n",
        "*Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **(out_channels=10, kernel_size=(3, 3), padding=(0, 0), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **(out_channels=22, kernel_size=(3, 3), padding=(1, 1), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : **(out_channels=65, kernel_size=(3, 3), padding=(2, 2), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : **(out_channels=7, kernel_size=(3, 3), padding=(6, 6), stride=(1, 1))**\n",
        "\n",
        "*Using a Kernel size of 5×5:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **(out_channels=10, kernel_size=(5, 5), padding=(1, 1), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **(out_channels=100, kernel_size=(5, 5), padding=(2, 2), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **(out_channels=23, kernel_size=(5, 5), padding=(3, 3), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **(out_channels=5, kernel_size=(5, 5), padding=(7, 7), stride=(1, 1))**\n",
        "\n",
        "*Using Kernel size of 5×3:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : **(out_channels=10, kernel_size=(5, 3), padding=(1, 0), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : **(out_channels=100, kernel_size=(5, 3), padding=(2, 1), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : **(out_channels=23, kernel_size=(5, 3), padding=(3, 2), stride=(1, 1))**\n",
        "* (c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : **(out_channels=5, kernel_size=(5, 3), padding=(7, 8), stride=(1, 1))**\n",
        "\n",
        "*Determine the kernel that requires the smallest padding size to make the following mappings possible:*\n",
        "\n",
        "* (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : **Kernel size (2, 4) with padding (0, 0) and stride (1, 1)**\n",
        "* (c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : **Kernel size (1, 1) with padding (0, 0) and stride (1, 1)**\n",
        "\n",
        "*Hint: Use* `conv_prac` *to visualize the shape changes*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6iWe-vrQ9GO"
      },
      "source": [
        "## ConvNet\n",
        "\n",
        "Now create a convolution network `ConvNet` that inherits `nn.Module`.\n",
        "The network should have 3 convolution layers (each layer should have 16 output channels):\n",
        "1. Conv Layer 1 should have a 6x6 kernel, no padding, and a stride of 2.\n",
        "2. Conv Layer 2 should have a 4x4 kernel, no padding, and a stride of 2.\n",
        "3. Conv Layer 3 should have a 3x3 kernel, no padding, and a stride of 1.\n",
        "\n",
        "The output of these layers should be a 3x3 image with 16 channels.\n",
        "You should flatten the image (you can use `.view()`, `torch.flatten()`, or `nn.Flatten()`) and then pass it through 2 linear layers:\n",
        "1. Linear Layer 1 should take the flattened image and map it to a vector with 16 features.\n",
        "2. Linear Layer 2 should map its vector to logits.\n",
        "\n",
        "Do not forget to add nonlinearities between the layers (do not add them to the last layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "a0_GBKlkQ9GO"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=6, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "\n",
        "        flattened_size = 16 * 3 * 3\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 16)\n",
        "        self.fc2 = nn.Linear(16, 10) # 10 output classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gynh4o0GQ9GP"
      },
      "source": [
        "Train you convolution network below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AeFfiCH4Q9GP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "collapsed": true,
        "outputId": "c5c815ab-eac3-4428-afd8-0c961c97283a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x786ee00b4f40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1664, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 1628, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3550674103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# TODO: Train your ConvNet for 2000 steps and set log_val_interval=50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3106985341.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, val_loader, optimizer, n_optimization_steps, log_val_interval)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_val_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-874595723.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(net, val_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 )\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     def _dispatch_in_python(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TODO: Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# TODO: Initalize your ConvNet, called 'conv_net'\n",
        "conv_net = ConvNet().to(device)\n",
        "\n",
        "# TODO: Create an Adam optimizer (lr=.001 works well)\n",
        "optimizer = torch.optim.Adam(conv_net.parameters(), lr=.001)\n",
        "\n",
        "# TODO: Train your ConvNet for 2000 steps and set log_val_interval=50\n",
        "train_losses, train_accuracies, val_losses, val_accuracies = train(conv_net, train_loader, val_loader, optimizer, 2000, 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpvG7fl1Q9GP"
      },
      "source": [
        "*Now* plot your training and validation loss on the same plot and plot your training and validation accuracies on the same plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "mC3GifTWQ9GP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "outputId": "edbf5222-e85f-4cca-c3ce-c4244a5b5b80"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_losses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4018059616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlgZfmwRQ9GP"
      },
      "source": [
        "Create a function that outputs the number of parameters in a network.\n",
        "Remember you can call `.parameters()` to recursively retrieve the `Parameter`s in a `Module`.\n",
        "You could then use `.shape` to figure out the number of parameters in a `Parameter` or you could flatten the parameter and get its length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "INzCWjI5Q9GP"
      },
      "outputs": [],
      "source": [
        "def get_n_net_params(net):\n",
        "    # TODO: Implement this function\n",
        "    n_params = 0\n",
        "    for param in net.parameters():\n",
        "        param_shape = param.shape\n",
        "        num_elements = 1\n",
        "        for dim in param_shape:\n",
        "            num_elements *= dim\n",
        "        n_params += num_elements\n",
        "    return n_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQR4_hPjQ9GQ"
      },
      "source": [
        "Print the number of parameters and the accuracy of your trained `MLP` and `ConvNet` networks and then write down below:\n",
        "- Which one is more accurate?\n",
        "- Which one is smaller?\n",
        "\n",
        "*Note: You can use `validation()` to get the accuracy of your network.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "498fb3f3"
      },
      "source": [
        "Print the number of parameters and the accuracy of your trained `MLP` and `ConvNet` networks and then write down below:\n",
        "- Which one is more accurate? **[Write your answer here based on the output]**\n",
        "- Which one is smaller? **[Write your answer here based on the number of parameters]**\n",
        "\n",
        "*Note: You can use `validation()` to get the accuracy of your network.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1208c72c"
      },
      "source": [
        "Print the number of parameters and the accuracy of your trained `MLP` and `ConvNet` networks and then write down below:\n",
        "- Which one is more accurate? **Based on my (potentially flawed) output, the MLP is more accurate (0.3312) than the ConvNet (0.1099) with the current training state.**\n",
        "- Which one is smaller? **The ConvNet is smaller (9514 parameters) than the MLP (118282 parameters).**\n",
        "\n",
        "*Note: You can use `validation()` to get the accuracy of your network.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wVCnQ1ccQ9GQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229e8a61-aae7-4a25-f658-2d05446724dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Parameters: 118282\n",
            "ConvNet Parameters: 9514\n",
            "MLP Validation Accuracy: 0.3309\n",
            "ConvNet Validation Accuracy: 0.1099\n"
          ]
        }
      ],
      "source": [
        "# Assuming MLP and ConvNet models (net and conv_net) have been initialized and potentially trained\n",
        "# Assuming validation function is available and works\n",
        "\n",
        "mlp_params = get_n_net_params(net)\n",
        "conv_net_params = get_n_net_params(conv_net)\n",
        "\n",
        "print(f\"MLP Parameters: {mlp_params}\")\n",
        "print(f\"ConvNet Parameters: {conv_net_params}\")\n",
        "\n",
        "try:\n",
        "    mlp_val_loss, mlp_val_acc = validation(net, val_loader)\n",
        "    print(f\"MLP Validation Accuracy: {mlp_val_acc:}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not get MLP accuracy: {e}\")\n",
        "    mlp_val_acc = None\n",
        "\n",
        "try:\n",
        "    conv_net_val_loss, conv_net_val_acc = validation(conv_net, val_loader)\n",
        "    print(f\"ConvNet Validation Accuracy: {conv_net_val_acc:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not get ConvNet accuracy: {e}\")\n",
        "    conv_net_val_acc = None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}